"""
Module de gestion du mod√®le de langage (LLM)
G√®re l'interaction avec Ollama (Mistral), Groq ou Mistral AI
"""

import os

# Imports conditionnels pour √©viter les erreurs de d√©pendances
try:
    from langchain_community.llms import Ollama
except ImportError:
    Ollama = None

try:
    from langchain_groq import ChatGroq
except ImportError:
    ChatGroq = None

try:
    from langchain_mistralai import ChatMistralAI
except ImportError:
    ChatMistralAI = None

from src.config import OLLAMA_BASE_URL, OLLAMA_MODEL, MISTRAL_API_KEY


class LLMManager:
    """
    Classe pour g√©rer le mod√®le de langage
    """
    
    def __init__(
        self, 
        model_name: str = OLLAMA_MODEL,
        base_url: str = OLLAMA_BASE_URL,
        temperature: float = 0.3,
    ):
        """
        Initialise le gestionnaire LLM
        
        Args:
            model_name: Nom du mod√®le
            base_url: URL de l'API (pour Ollama)
            temperature: Temp√©rature de g√©n√©ration
        """
        self.model_name = model_name
        self.base_url = base_url
        self.temperature = temperature
        self.llm = self._initialize_llm()

    def _initialize_llm(self):
        """
        Initialise le fournisseur LLM appropri√©
        """
        # 1. V√©rifier si Groq est disponible
        groq_api_key = os.getenv("GROQ_API_KEY")
        if groq_api_key and ChatGroq:
            print("üöÄ Utilisation de Groq Cloud API")
            return ChatGroq(
                api_key=groq_api_key,
                model_name="llama-3.1-8b-instant",
                temperature=self.temperature
            )

        # 2. V√©rifier si Mistral AI API est disponible
        if MISTRAL_API_KEY and ChatMistralAI:
            print("üöÄ Utilisation de Mistral AI API")
            return ChatMistralAI(
                api_key=MISTRAL_API_KEY,
                model="mistral-medium",
                temperature=self.temperature
            )

        # 3. Par d√©faut : Ollama (local)
        if Ollama:
            print(f"ü§ñ Utilisation d'Ollama local : {self.model_name}")
            try:
                return Ollama(
                    model=self.model_name,
                    base_url=self.base_url,
                    temperature=self.temperature
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur Ollama : {e}")
        
        # Si rien ne marche
        print("‚ùå Aucun moteur LLM disponible. V√©rifiez les installations.")
        return None

    def generate(self, prompt: str) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'un prompt
        """
        try:
            if hasattr(self.llm, "invoke"):
                response = self.llm.invoke(prompt)
                return response.content if hasattr(response, "content") else str(response)
            else:
                return self.llm(prompt)
        except Exception as e:
            print(f"‚ùå Erreur de g√©n√©ration : {e}")
            return f"Erreur : Impossible de g√©n√©rer une r√©ponse. D√©tails : {str(e)}"
    
    def get_llm(self):
        """
        Retourne l'instance LLM pour utilisation avec LangChain
        """
        return self.llm


def create_rag_prompt(context: str, question: str) -> str:
    """
    Cr√©e un prompt structur√© pour le RAG
    """
    prompt = f"""[INSTRUCTION]
Tu es un assistant intelligent pour une entreprise de t√©l√©communication au S√©n√©gal nomm√©e YAS.
Ta mission est de r√©pondre aux questions des employ√©s en te basant UNIQUEMENT sur les documents internes fournis ci-dessous.

R√àGLES IMPORTANTES :
- R√©ponds en fran√ßais de mani√®re claire et professionnelle
- Base-toi UNIQUEMENT sur le contexte fourni
- Si l'information n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents disponibles"
- Cite toujours la source de l'information (nom du document)
- Sois pr√©cis et factuel
- Structure ta r√©ponse avec des listes √† puces ou num√©rot√©es si appropri√©

[CONTEXTE]
{context}

[QUESTION]
{question}

[R√âPONSE]
"""
    return prompt


# Fonction utilitaire pour obtenir le LLM
def get_llm(model_name: str = OLLAMA_MODEL, temperature: float = 0.3):
    """
    Fonction utilitaire pour obtenir une instance LLM
    """
    manager = LLMManager(model_name=model_name, temperature=temperature)
    return manager.get_llm()


if __name__ == "__main__":
    # Test du module
    print("üß™ Test du module LLM")
    try:
        manager = LLMManager()
        test_prompt = "Bonjour, peux-tu te pr√©senter en une phrase ?"
        print(f"R√©ponse : {manager.generate(test_prompt)}")
    except Exception as e:
        print(f"‚ùå √âchec : {e}")
