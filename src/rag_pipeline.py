"""
Module du pipeline RAG complet
Orchestre l'ensemble du processus : recherche + g√©n√©ration
"""

from typing import List, Dict
# from langchain.chains import RetrievalQA (unused)
from langchain_core.prompts import PromptTemplate
from src.vectorstore import load_vectorstore
from src.llm import get_llm, create_rag_prompt
from src.config import TOP_K_RETRIEVAL


class RAGPipeline:
    """
    Classe principale orchestrant le pipeline RAG complet
    """
    
    def __init__(self, top_k: int = TOP_K_RETRIEVAL):
        """
        Initialise le pipeline RAG
        
        Args:
            top_k: Nombre de documents √† r√©cup√©rer lors de la recherche
        """
        self.top_k = top_k
        
        print(f"üöÄ Initialisation du pipeline RAG (top-k={top_k})")
        
        # Charger la base vectorielle (avec auto-construction si n√©cessaire)
        print(f"üìö Chargement de la base vectorielle...")
        if not VECTORSTORE_DIR.exists() or not list(VECTORSTORE_DIR.glob("*.faiss")):
            print("‚ö†Ô∏è Base vectorielle manquante. Construction en cours...")
            from src.build_vectorstore import build_vectorstore
            build_vectorstore()
            
        self.vectorstore_manager = load_vectorstore()
        self.retriever = self.vectorstore_manager.get_retriever(k=top_k)
        
        # Charger le LLM
        print(f"ü§ñ Chargement du LLM...")
        self.llm = get_llm()
        
        print(f"‚úÖ Pipeline RAG initialis√© avec succ√®s")
    
    def _format_prompt(self, context: str, question: str, history: List[Dict] = None) -> str:
        """
        Cr√©e le prompt pour le RAG avec historique
        """
        history_text = ""
        if history:
            history_text = "\n[HISTORIQUE DE CONVERSATION]\n"
            for msg in history[-5:]:  # Garder les 5 derniers √©changes
                role = "Utilisateur" if msg["role"] == "user" else "Assistant"
                history_text += f"{role}: {msg['content']}\n"
        
        prompt = f"""[INSTRUCTION]
Vous √™tes l'Assistant Virtuel de YAS (T√©l√©com S√©n√©gal), expert, chaleureux et professionnel.
Votre r√¥le est d'agir comme un v√©ritable agent du service client.

R√àGLES ABSOLUES :
1. Basez-vous UNIQUEMENT sur les [CONTEXTE] fournis ci-dessous. N'inventez RIEN.
2. Si la r√©ponse n'est pas dans le contexte, dites poliment que vous ne trouvez pas l'information et proposez de contacter le service client au 200.
3. Soyez courtois, empathique et direct (style "Service Client Premium").
4. Utilisez le vouvoiement.
5. Formatez la r√©ponse avec des puces ou du gras pour la lisibilit√© si n√©cessaire.

{history_text}

[CONTEXTE]
{context}

[QUESTION CLIENT]
{question}

[R√âPONSE SERVICE CLIENT]
"""
        return prompt
    
    def query(self, question: str, history: List[Dict] = None) -> Dict:
        """
        Pose une question au syst√®me RAG avec historique
        
        Args:
            question: Question de l'utilisateur
            history: Liste de dictionnaires {"role": "user"/"assistant", "content": "..."}
            
        Returns:
            Dictionnaire contenant la r√©ponse et les sources
        """
        print(f"\n‚ùì Question : {question}")
        
        try:
            # 1. Recherche des documents pertinents
            source_documents = self.retriever.get_relevant_documents(question)
            
            # 2. Pr√©paration du contexte
            context = "\n\n".join([doc.page_content for doc in source_documents])
            
            # 3. Cr√©ation du prompt
            prompt = self._format_prompt(context, question, history)
            
            # 4. G√©n√©ration de la r√©ponse
            if self.llm is None:
                answer = "Le service d'IA (LLM) est actuellement indisponible. Veuillez v√©rifier la connexion ou les cl√©s API."
            elif hasattr(self.llm, "invoke"):
                response = self.llm.invoke(prompt)
                answer = response.content if hasattr(response, "content") else str(response)
            else:
                answer = self.llm(prompt)
            
            print(f"‚úÖ R√©ponse g√©n√©r√©e avec {len(source_documents)} sources")
            
            return {
                "question": question,
                "answer": answer,
                "sources": source_documents
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse : {e}")
            return {
                "question": question,
                "answer": f"Erreur : {str(e)}",
                "sources": []
            }
    
    def query_with_details(self, question: str, history: List[Dict] = None) -> Dict:
        """
        Pose une question et retourne des d√©tails enrichis
        """
        result = self.query(question, history)
        
        # Enrichir avec les d√©tails des sources
        sources_details = []
        for doc in result["sources"]:
            sources_details.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })
        
        result["sources_details"] = sources_details
        
        return result
    
    def format_response(self, result: Dict) -> str:
        """
        Formate la r√©ponse pour affichage
        """
        formatted = f"{result['answer']}\n\n"
        
        if result['sources']:
            formatted += f"**Sources :**\n"
            seen_sources = set()
            for doc in result['sources']:
                source_name = doc.metadata.get('source', 'Document')
                # Nettoyer le chemin pour n'avoir que le nom du fichier
                from pathlib import Path
                source_name = Path(source_name).name
                
                if source_name not in seen_sources:
                    formatted += f"- {source_name}\n"
                    seen_sources.add(source_name)
        
        return formatted


# Fonction utilitaire pour utilisation rapide
def ask_question(question: str) -> Dict:
    """
    Fonction utilitaire pour poser une question rapidement
    
    Args:
        question: Question de l'utilisateur
        
    Returns:
        Dictionnaire avec r√©ponse et sources
    """
    pipeline = RAGPipeline()
    return pipeline.query(question)


if __name__ == "__main__":
    # Test du module
    print("üß™ Test du pipeline RAG")
    
    try:
        # Initialiser le pipeline
        pipeline = RAGPipeline()
        
        # Poser une question de test
        test_question = "Quelles sont les principales offres disponibles ?"
        result = pipeline.query(test_question)
        
        # Afficher le r√©sultat format√©
        print("\n" + "="*60)
        print(pipeline.format_response(result))
        print("="*60)
        
    except Exception as e:
        print(f"‚ùå Le test a √©chou√© : {e}")
        print(f"üí° Assurez-vous que :")
        print(f"   1. Ollama est d√©marr√© : ollama serve")
        print(f"   2. Le mod√®le mistral est t√©l√©charg√© : ollama pull mistral")
        print(f"   3. La base vectorielle existe (ex√©cutez build_vectorstore.py)")
