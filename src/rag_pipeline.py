"""
Module du pipeline RAG complet
Orchestre l'ensemble du processus : recherche + g√©n√©ration
"""

from typing import List, Dict
# from langchain.chains import RetrievalQA (unused)
from langchain_core.prompts import PromptTemplate
from src.vectorstore import load_vectorstore
from src.llm import get_llm, create_rag_prompt
from src.config import TOP_K_RETRIEVAL, VECTORSTORE_DIR


class RAGPipeline:
    """
    Classe principale orchestrant le pipeline RAG complet
    """
    
    def __init__(self, top_k: int = TOP_K_RETRIEVAL):
        """
        Initialise le pipeline RAG
        
        Args:
            top_k: Nombre de documents √† r√©cup√©rer lors de la recherche
        """
        self.top_k = top_k
        
        print(f"üöÄ Initialisation du pipeline RAG (top-k={top_k})")
        
        # Charger la base vectorielle (avec auto-construction si n√©cessaire)
        from src.config import VECTORSTORE_DIR
        print(f"üìö Chargement de la base vectorielle...")
        if not VECTORSTORE_DIR.exists() or not list(VECTORSTORE_DIR.glob("*.faiss")):
            print("‚ö†Ô∏è Base vectorielle manquante. Construction en cours...")
            from src.build_vectorstore import build_vectorstore
            build_vectorstore()
            
        self.vectorstore_manager = load_vectorstore()
        self.retriever = self.vectorstore_manager.get_retriever(k=top_k)
        
        # Charger le LLM
        print(f"ü§ñ Chargement du LLM...")
        self.llm = get_llm()
        
        print(f"‚úÖ Pipeline RAG initialis√© avec succ√®s")
    
    def _format_prompt(self, context: str, question: str, history: List[Dict] = None) -> str:
        """
        Cr√©e le prompt pour le RAG avec historique
        """
        history_text = ""
        if history:
            for msg in history[-6:]:  # Prendre les 3 derniers tours (user+assistant)
                role = "Client" if msg["role"] == "user" else "YAS"
                history_text += f"{role}: {msg['content']}\n"
        
        prompt = f"""[INSTRUCTION]
Vous √™tes l'Assistant Virtuel de YAS (T√©l√©com S√©n√©gal), expert, chaleureux et professionnel.
Votre r√¥le est d'agir comme un v√©ritable agent du service client.

R√àGLES D'OR :
1. Basez-vous UNIQUEMENT sur le [CONTEXTE] pour r√©pondre.
2. Si l'info n'est pas l√†, redirigez poliment vers le 200. N'inventez JAMAIS de tarifs ou de proc√©dures.
3. Rapportez-vous √† l'HISTORIQUE ci-dessous si le client pose une question de suivi (ex: "Et le prix ?").
4. Style : Premium, empathique, structur√©.

HISTORIQUE R√âCENT :
{history_text if history_text else "Premier contact."}

[CONTEXTE]
{context}

[QUESTION CLIENT]
{question}

[R√âPONSE SERVICE CLIENT]
"""
        return prompt
    
    def query(self, question: str, history: List[Dict] = None) -> Dict:
        """
        Pose une question au syst√®me RAG avec historique
        
        Args:
            question: Question de l'utilisateur
            history: Liste de dictionnaires {"role": "user"/"assistant", "content": "..."}
            
        Returns:
            Dictionnaire contenant la r√©ponse et les sources
        """
        print(f"\n‚ùì Question : {question}")
        
        try:
            # 1. Recherche des documents pertinents
            source_documents = self.retriever.invoke(question)
            
            # 2. Pr√©paration du contexte
            context = "\n\n".join([doc.page_content for doc in source_documents])
            
            # 3. Cr√©ation du prompt
            prompt = self._format_prompt(context, question, history)
            
            # 4. G√©n√©ration de la r√©ponse
            if self.llm is None:
                answer = "Le service d'IA (LLM) est actuellement indisponible. Veuillez v√©rifier la connexion ou les cl√©s API."
            elif hasattr(self.llm, "invoke"):
                response = self.llm.invoke(prompt)
                answer = response.content if hasattr(response, "content") else str(response)
            else:
                answer = self.llm(prompt)
            
            print(f"‚úÖ R√©ponse g√©n√©r√©e avec {len(source_documents)} sources")
            
            return {
                "question": question,
                "answer": answer,
                "sources": source_documents
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse : {e}")
            return {
                "question": question,
                "answer": f"Erreur : {str(e)}",
                "sources": []
            }
    
    def query_with_details(self, question: str, history: List[Dict] = None) -> Dict:
        """
        Pose une question et retourne des d√©tails enrichis
        """
        result = self.query(question, history)
        
        # Enrichir avec les d√©tails des sources
        sources_details = []
        for doc in result["sources"]:
            sources_details.append({
                "content": doc.page_content,
                "metadata": doc.metadata
            })
        
        result["sources_details"] = sources_details
        
        return result
    
    def format_response(self, result: Dict) -> str:
        """
        Formate la r√©ponse pour affichage (Sans les sources pour la visibilit√©)
        """
        return result['answer']


# Fonction utilitaire pour utilisation rapide
def ask_question(question: str) -> Dict:
    """
    Fonction utilitaire pour poser une question rapidement
    
    Args:
        question: Question de l'utilisateur
        
    Returns:
        Dictionnaire avec r√©ponse et sources
    """
    pipeline = RAGPipeline()
    return pipeline.query(question)


if __name__ == "__main__":
    # Test du module
    print("üß™ Test du pipeline RAG")
    
    try:
        # Initialiser le pipeline
        pipeline = RAGPipeline()
        
        # Poser une question de test
        test_question = "Quelles sont les principales offres disponibles ?"
        result = pipeline.query(test_question)
        
        # Afficher le r√©sultat format√©
        print("\n" + "="*60)
        print(pipeline.format_response(result))
        print("="*60)
        
    except Exception as e:
        print(f"‚ùå Le test a √©chou√© : {e}")
        print(f"üí° Assurez-vous que :")
        print(f"   1. Ollama est d√©marr√© : ollama serve")
        print(f"   2. Le mod√®le mistral est t√©l√©charg√© : ollama pull mistral")
        print(f"   3. La base vectorielle existe (ex√©cutez build_vectorstore.py)")
